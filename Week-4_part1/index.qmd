---
title: "Module 3.2: Information Retrieval: Representations and Models"
subtitle: "LIS 5043: Organization of Information"
author: 
  - Dr. Manika Lamba
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      theme: whiteboard
      buttons: true
    preview-links: true
    controls: true
    progress: true
    show-notes: separate-page
    logo: images/ou.png
    css: styles.css
editor: 
  markdown: 
    wrap: 72
---

## Model of Information Retrieval {.smaller}

![](images/1.png){fig-align="center" width="80%"}

::: notes
In Module 3.2 we’re going to talk about information retrieval (IR) from
different perspectives. We’re first going to talk about the idea of
representation within information systems, and how it affects
information retrieval. Then we will look at some different information
retrieval models that are present in today’s systems, as well as look at
some of those in earlier systems.

To begin our discussion, here is the recap of the model from Module 2 to
help you think through the idea of multiple representations and how they
interact to connect users to information.

Representation A is the user’s information need, which the user enters
into a system using terms that they believe represent this need.

Representation B is the representation created as a surrogate for the
information object by a cataloger or indexer. Often the cataloger will
use a set of authorized terms, called a controlled vocabulary to
represent the subjects, but also a set of standards that tell them how
to describe the object within the representation.

Both are entered into a system.

The retrieval component of the information system matches these two
representations and brings back a subset of representations that the
system contains that are representative of the words in the users
queries. (We won’t go into the mechanics of searching at this point, or
how the retrieval mechanisms work within various systems.)

The user then chooses a representation to review to see if it meets
their information need or not.
:::

## Representation: Definition

-   A system for choosing or highlighting some characteristics
    (attributes), together with a specification of the rules for
    selection (codes)

-   This implies a trade-off: if some characteristics are highlighted,
    other characteristics are left behind

::: notes
Next, let’s begin by defining ‘representation’ within the broader
context. Now you’ll recall, we talked about representation in Module 2,
but let’s look even more specifically at some of the issues related to
representation within IR systems.

Representation can be thought of as a system for choosing or
highlighting some characteristics, or attributes – or general
characteristics -- together with the specification of the rules for
selection and the code, meaning that we should understand a little bit
about how the system is structured and how data exists within the
system’s records.

This selection (or surrogation ) process implies a trade - off of some
kind. If some characteristics are highlighted, then we have to assume
that other characteristics are left behind.

So, for example, in library catalogs we made decisions on what
attributes of the information objects in our collection are important to
describe in our records for users. In early catalog records these
attributes included, Title, Author, Subject, and Classification.

In order to create those representations, the card catalog records, we
had to decide which characteristics to highlight and which to leave
behind.
:::

## More Definitions {.smaller}

-   `ENTITES`: objects or concepts
-   `ATTRIBUTES`: characteristics of entities
    -   `DIACHRONIC`: stable across time
    -   `SYNCHRONIC`: changes across time

::: notes
Okay, let’s also look at some other definitions that we need to
consider.

In terms of representations, we have to consider the term of ‘entities.’
Entities are those objects, or even concepts, that we’re going to
represent.

‘Attributes’ are the general characteristics of information objects. So,
we can also think of them as characteristics of the entities that we’re
representing. There are different types of attributes; ‘diachronic’ and
‘synchronic’ attributes.

Diachronic attributes are those that remain stable across time. In other
words, they do not change, and their meanings and uses also do not
change.

And then there are synchronic attributes, and those are attributes that
change across time. The change could be as simple as the use of a
particular term or concept within a specific language.
:::

##  {.smaller}

![](images/2.png){fig-align="center" width="80%"}

::: notes
On this slide I have modelled the idea of how *agency* plays a role in
the information representation and retrieval process.

For example, we have an agency, which might be your local cataloging
department, or it could be an indexing service within a database. That
agency has a *document representation rule* , meaning that they have
predefined the structure in that particular system . And then they’ve
also made choices in terms of controlled vocabulary use, whether or not
they use a controlled vocabulary or the specific one that they do use.
And they may also have a *question representation rule* , meaning that a
user has to input our question into the system using a specific format
or syntax and it will be matched to the representations using a specific
algorithm.

Also, part of this model is the *retrieval mechanism rule* . That
particular system has a structure, such as the file structure or field
structure. The retrieval mechanism rule would also include the types of
data in the fields, how that data is retrieved, how it’s indexed within
a system – whether or not a field is searchable, and then also the
algorithms that match the user’s query terms to the representations
(records) within the system.
:::

## Human Indexing

-   Diachronic Attributes (do not change)
    -   *author, title, publisher, number of pages*
-   Only most general thought of users
-   Rules not evident to users
-   *Great vagueness & Generality resting on a foundation of shiftiing
    quicksand*

::: notes
There are several factors related to human indexing of particular
documents within collections. As I’ve said, we have diachronic
attributes, and those are the ones that don’t change. They’re always the
same when representing something in the system. Examples of diachronic
attributes would be author, title, publisher, number of pages.

The problem is that oftentimes representation systems have only the most
general thought about users. We’re designing this system for retrieval,
and the different agencies have different specifications for how or what
they want to be retrievable within their system. But we’re oftentimes
unaware of how the users use those particular systems or how they search
for items within the systems. Another problem that comes in with human
indexing is that the rules in which we create representations are not
evident to our users. How many times have you used a library catalog,
only to have results come back that are really confusing – with
abbreviations you don’t understand or no information on how to access
the resources?

So, again, there are different aspects of representation that are not
evident to users or are confusing to users.

When we’re thinking about representation and the different factors from
a human side that impact retrieval, “there’s a great vagueness and
generality resting on a foundation of shifting quicksand.”

Because if you think back to the IR/representation model that we looked
at earlier, users are only concerned with one side of the model; they’re
concerned with how they enter their information need or their query into
the system. As the person who is creating the representation, we have to
be concerned with both sides. We have to know the rules for creating the
representation, including the structure of the system, but we also have
to understand how our users are interacting with and/ or searching
within that particular system.
:::

## Requirements for Successful Retrieval

::: r-stack
![](images/3.png){.fragment width="250" height="250"}

![](images/4.png){.fragment width="500" height="400"}

![](images/5.png){.fragment width="600" height="400"}

![](images/6.png){.fragment width="700" height="500"}

![](images/7.png){.fragment width="750" height="500"}

![](images/8.png){.fragment width="800" height="500"}
:::

::: notes
Image 1: Okay, let’s look at some requirements for a successful
retrieval. We have a collection. It is shown on this slide with these
various different symbols, which may or may not be useful to us as a
user. `(Click Next)`

Image 2: Within our collection, there is at least one document, based on
its representation that might be useful to us. `(Click Next)`

Image 3: The user has some idea of what kind of information or
information resource would be useful to them in helping to satisfy an
information need. So, they input that information into the search
mechanism of the information retrieval system with the hopes of enacting
a match between their search terms and the terms used within the system
to represent the objects on that particular topic, subject, or by a
particular author.

So, we can say that retrieval was successful because at least one of the
terms, the search terms that the user input into the system, matched
with the terms in the representation and brought back the document or
resource that the user thought might be useful. `(Click Next)`

Image 4: So, we have a patron! `(Click Next)`

Image 5: With a very specific information need. `(Click Next)`

Image 6: When the indexer created this representation, they picked the
four symbols that you see in the pink bar. And these symbols are
represented within this document. Now, again, what they’ve done is
chosen to *highlight* specific attributes of this object for their
representation. So, we can see four different aspects that were probably
more important topics within this object, and that’s what the
cataloger/indexer chose to represent. Also, they might be constrained by
their system parameters or local practice as to how many concepts they
can represent instead of representing every concept within the document.

What’s also not known to the user at this point are the conventions
(local practice rules in the cataloging department) and the different
choices that the indexer must make.
:::

## 

![](images/10.png){fig-align="center" width="80%" height="80%"}

::: notes
Part of the problem also is that the indexer and the patron often do not
use the same code.

Or in this case, we can think of the controlled vocabulary, Library of
Congress Subject Headings (LCSH), as the code that we use to represent
subjects within our particular representations or records within our
system.

Patrons in libraries do not usually have an understanding about what the
four big, red books, or LCSH, contain and how they can be used in
searching. And oftentimes, especially today in the automated world in
which we live, we don’t provide the LSCH in online or print form to our
users as a potential source of topics or search terms.
:::

# 

::: columns
::: {.column width="60%"}
`Indexer has selected (perhaps among others) the concept that the patrons will want`
:::

::: {.column width="40%"}
::: r-stack
![](images/11.png){.fragment width="250" height="250"}

![](images/12.jpeg){.fragment width="500" height="400"}
:::
:::
:::

::: notes
Image 1: It really becomes a bit of a guessing game for the indexer;
they’ve selected, perhaps among others, the concept(s) that they think
the patron will want or that the patron will use when they’re searching
for documents about this particular item in the collection.
`(Click Next)`

Image 2: Now, if there’s a match between the terms or the concepts that
this indexer or cataloger has chosen and the patron has used in their
search terms, there is successful retrieval.
:::

## What If?

-   `Indexer picks a different topic`
-   `Indexer and patron use different terms for the same concept`
-   `Patrons cannot articulate just what the question state is`

::: notes
But! What happens if there’s no match? We can think about this as a
dance.

So, what if the indexer picks a different topic?

What if the indexer and the patron use different terms for the same
concept, such as if the indexer uses ‘ automobiles’ and the patron uses
‘ cars’ ?

What if the patron can’t articulate just what their question is or what
their question state might be, meaning that they’re not entirely sure
what kinds of information they’re looking for to satisfy this particular
need.
:::

## The Dance

::: columns
::: {.column width="50%"}
**Indexer**

describes doc

predicts use
:::

::: {.column width="50%"}
**Patron**

describes doc

predicts doc
:::
:::

::: notes
The dance begins between the indexer and the patron.

The indexer describes the document, where the patron is trying to
predict the document that they need that will help them resolve an
information need.

The indexer predicts use of the particular items that they’re
representing.

The patron, on the other hand, has to somehow describe how they’re going
to use the document or the object in your collection to satisfy their
information need.

So, if you look at it from this perspective, you can see that there are
a lot of places where the retrieval will break down. If the indexer and
the patron do not correspond in terms of how the document is described
and how they’ll use the documents or how the indexer believes the
document will be used versus how the patron is describing their use,
then there can be problems in retrieving documents from the collection.
:::

## Some Important Questions

-   `What patron attributes can we know?`
-   `What document attributes can we know?`
-   `How can we use this knowledge to open the bottleneck between patrons in need and the documents that might be of use?`

::: notes
In Module 3.1 we talked about users and what we can know about users.
So, at this point, let’s also consider “What patron attributes can we
know?

We can know about their age, we can know about their economic status
potentially, depending on where they’re accessing our collection. We can
know about their gender. We might know about the particular use. So,
there are different aspects of patrons we can know about. Now that we’re
in an online environment, knowing who our users are is very difficult,
and our representations reflect that difficulty by becoming more general
in nature.

We also can know what document attributes are useful within our systems.
And how we determine this is oftentimes through user studies, in which
we talk to users about how they use our systems, or how they use
collections, or how they conduct searching within different systems.

So, how can we use this knowledge to open up what we would consider the
‘bottleneck’ between patrons with a particular information need and the
documents that might be of use to them?
:::

## Indexing Factors Affecting IR Performance {.smaller}

::: columns
::: {.column width="50%"}
1.  [Indexing]{style="color: orange;"}

------------------------------------------------------------------------

2.  [Type of Knowledge]{style="color: purple;"}

------------------------------------------------------------------------

3.  [Effective/Cognitive]{style="color: red;"}
:::

::: {.column width="50%"}
1.1 [Consistency]{style="color: orange;"}

1.2 [Subject Expertise]{style="color: orange;"}

1.3 [Indexing Expertise]{style="color: orange;"}

------------------------------------------------------------------------

2.1 [Searching Experience]{style="color: purple;"}

2.2 [Domain Knowledge]{style="color: purple;"}

------------------------------------------------------------------------

3.1 [Motivation Level]{style="color: red;"}

3.2 [Emotional State]{style="color: red;"}
:::
:::

::: notes
There are other indexing factors that do effect information retrieval
(IR) performance, and some of these are related back to human factors.

For example, within the indexing process and resulting index , there is
a problem that we call ‘inter - indexer consistency,’ or ‘inter -
indexer inconsistency,’ meaning that there is a low percentage of
consistency between any two indexers when they’re creating
representations.

There is also the problem with subject expertise. People with higher
domain knowledge, generally, are better indexers in that particular
area.

And also indexers with more experience tend to know the codes or the
controlled vocabularies better, as well as how representations are
structured.

Then there are also factors from the user’s side such as the types and
levels of knowledge, the user’s search experience and domain knowledge;
these factors can also play a role in retrieval performance.

There are also effective and cognitive factors, such as a person’s
motivation level and emotional state as they’re looking for information.
:::

## Other Factors to Consider

-   `Use of Standards/Rules (code)`
    -   Depends on form of index/abstract
    -   Depends on criteria of employer
        -   Pages allocated
        -   Format used
        -   Order
-   `Depends on Resources/Audience`

::: notes
One final set of factors to consider – and I’m sure that you can also
think of others – is that the indexer or cataloger has specific
standards and rules, or what we might refer to as a code, that have to
be followed when they create representations.

We’ll be covering the codes or standards we use in library cataloging or
indexing in more depth coming in future modules 6 and 7. Those standards
and rules depend on the form of the index or the abstract or the record
being created; they also depend on criteria of employers, such as how
many pages are allocated for an index, what format can be used, how will
the information be structured and displayed to the users, what is the
particular order of the elements of the record, etc.

Again, all of this depends upon which type of organizing structure is
being developed (MARC record, back of the book index, etc.), as well as
the audience or the users.
:::

## Information Retrieval {.smaller}

> `A process in which sets of records or documents are searched to find items which may help to satisfy the information need`

-   IR is concerned with:

    -   `representation`
    -   `storage`
    -   `organization`
    -   `accessing of information objects`

::: notes
Okay, now that we have talked a little bit about representation and some
of the issues related to representation within IR systems, let’s step
back a bit and talk about IR, as well as IR models.

IR has been defined very basically as a process in which sets of records
or documents are searched to find items, which may help to satisfy the
user’s information needs.

Now, we know from our discussions throughout class that IR is a lot more
complicated than has been defined here.

IR is concerned with several different processes as well as how the
system is structured. It’s concerned with

-   representation from both a user’s side, or
-   how the user represents their need within a system mechanism, and
    from the computerized side, or
-   from the system’s side of how items in the collection are
    represented within the system,
-   how the system is set up and structured, and
-   how the cataloger or indexer has represented those objects within
    that system structure.

IR is also concerned with storage of the representations from the system
side (and how the system is structured) and the organization of those
representations, meaning how the field structure is devised, the system
search algorithms, and the different IR models in use to access the
information objects successfully.
:::

## Information Retrieval (Cont.) {.smaller}

-   Information retrieval concerns a range of concepts
    -   `User Group`
        -   types of knowledge
        -   context & information environment
    -   `Information Need`
    -   `Information Sources`
    -   `Information System`
        -   system capabilities/IR techniques used
        -   how information organized
    -   `Results of the Query`
    -   `User Selection & Evaluation (Relevance)`

::: notes
IR concerns a wide range of concepts. In Module 3.1 we talked about
users, what we have learned about users in our research, and what we can
learn about users – related to their types and levels of knowledge – as
well as the different variables the come into play that are related to
context and the information environment in which they’re conducting
their searches. We also talked about different information needs and
information sources.

We’re going to focus this time more about the **information system**. We
are going to look at the system capabilities and the IR techniques or
models being used, as well as how that impacts how information is
organized and retrieved. Another component here, of course, is the
results of the query and the user selection.
:::

## Information Retrieval (Cont.) {.smaller}

-   The central problem of IR is how to represent documents for
    retrieval
-   To be more successful, document representation must be used in ways
    similar to the ways ordinary language is used
    -   document representations should take context into account
    -   document representations should take users into account

::: notes
The central problem of IR then is how to represent documents for
retrieval. Our field has done a lot of research on users and their
information behavior, as we talked about in Module 3.1. We can also
think about it from the system’s side of how documents are represented
by human agencies as well as computerized agency within the system.

To be more successful, we’ve found in our research, document
representation must be similar to the way that ordinary language is
used, which, again, presents some problems.

We’ll talk later in this lecture about the natural language processing
model of IR and some of the issues associated with that.

But document representation should take context into account, or the
domain of the system into account, as well as the users of that system.
:::

## Information Retrieval (Cont.) {.smaller}

-   `Most IR is based on techniques introduced in the 1960's`
    -   Primarily text-based retrieval
    -   Makes use of inverted indexes and index terms
-   `IR is no longer just a library problem`
    -   Used in businesses, everyday settings
    -   Used in search engines
-   `As a result of these evolved uses high standards of retrieval are expected by users`

::: notes
IR and the models that we’ll cover in this lecture, both the classical
and the more advanced models being developed and being used today, were
introduced in the 1960’s. The retrieval within those models was and
still remains primarily textual based. What I mean by ‘textual based’ is
that the systems were being designed primarily to retrieve text - based
documents within sets of different document collections using text -
based representations.

IR systems also make use of inverted indexes and index terms, especially
within our online databases. Even search engines have inverted indexes
as the basis for retrieval.

It’s also important to note that information retrieval is no longer just
a library problem. People do not just retrieve documents within library
systems any longer; IR is used in multiple different contexts as well as
in everyday homes. People are using the web, and the penetration of web
access within the home environment is pretty high within the United
States, though it does vary. There are definitely digital divide issues;
however, users can also access the web for everyday use in libraries and
other organizations, and of course, using smart phones and other
devices. And around the world we know that access to the web is also at
varying levels.

As a result of these more evolved uses – we’re no longer just using
databases to do research – users have very high standards of retrieval
and of our retrieval systems. And oftentimes they come away from a
library OPAC being very unsatisfied with the results. Some of these
expectations can be attributed to the system, but also to the user’s
misunderstandings about how that particular system functions.
:::

## Taxonomy of IR Techniques

-   `We can divide IR techniques into basic classes`
    -   **Exact Match**: *where the set of retrieved documents contains
        only documents whose representations match exactly with the
        query*
    -   **Partial Match**: *where there is some matching that occurs,
        but it is not exact, although some of the documents may be exact
        matches to the query*

::: notes
At the heart, we can divide information retrieval techniques into two
basic classes: **exact match** and **partial match**. Systems today
still have these techniques in place.

Exact match is where the set of retrieved documents, contains *only*
documents whose representations – those of the indexer within the system
and those of the user in their search queries – *match* exactly when an
IR transaction is taking place.

For example, in an exact match system, if I put in the terms,
‘information’ and ‘data,’ my system documents are only going to retrieve
those items that include ‘information’ and ‘data,’ and they will leave
out any other types of materials that might be related even though they
could be potential matches.

In a partial match model, this is where some matching occurs, the terms
are not necessarily exactly the same , some of the documents may be
exact matches to the query, but then others might be what we considered
‘partial’ or ‘low relevance matching.’ Both of these two techniques are
still present in systems today.
:::

## Traditional IR Model {.smaller}

#### **`Simple Match Model`**

> **Request = Information Data**

Document A = data, information

Document B = data, information

Document C = information, retrieval

`Advantages`: simple process; widespread; familiar

`Disadvantages`: single descriptor requests less effective in large
databases

::: notes
One of the classical IR models is the *‘simple match model’* or a *‘best
match model’* that is part of an IR model.

For example, the user has a request, which is the information that’s
needed, and that matches to the documents in our collection. So, again
if we use that same example of ‘data’ and ‘information,’ if document A
and document B have some reference to ‘data’ and ‘information’ within
that system’s representation, within their index record, or within their
bibliographic record, then we’re going to have a successful match.

The problem with simple match models is that it usually uses only a few
query terms which becomes less effective in large databases.

The advantage is that it’s a simple process; it’s very wide spread and
familiar to our users. They enter terms into a search engine or into a
fielded search in a database, and they get back results.

But as I said, it becomes even less effective in larger databases where
we’re searching multiple collections with thousands of documents within
the collections.
:::

## Boolean IR Model {.smaller}

-   Boolean Retrieval
    -   one step above the basic model
-   Named after the creator, *George Boole*, of Boolean algebra, around
    1850
-   Most familiar IR technique used in OPAC's and online databases
-   Uses `AND`, `OR`, and `NOT` to allow more complex queries to the IR
    system
-   Works with that what is called `Set Theory`

::: notes
One of the most *prevalent* classical IR models is what’s called
‘Boolean retrieval,’ which is one step above the basic model that I just
described.

Boolean retrieval is present in most systems today that you’ve used,
from library OPACs, to databases, to search engines. ‘Boolean’ was named
for its creator, George Boole,when he developed Boolean algebra around
1850.

As I noted, it’s probably the most familiar IR technique, but it’s also
one of the least understood techniques because the user has to
manipulate the query in some way, where other techniques might work
behind the interface and the user doesn’t have as much control over how
the query is processed by the system, but they also don’t know what the
system is doing.

Boolean uses the AND, OR, or NOT operators to allow more complex queries
within the system. A user can structure queries in many different ways,
and systems may also process Boolean operators in a different order,
meaning that those search operators of AND, OR, or NOT are treated in a
specific order within that system.

We’ll talk more about operator order and Boolean searching in the next
lecture (Module 3.3) when we look at specific search structures and
creating good search strings within IR systems.

Boolean also works with what is called ‘set theory,’ which is a binary
approach – an item either belongs to a set or it doesn’t belong to a
set. This is both an advantage and a limitation of the IR model.
:::

## Boolean IR Model (Cont.) {.smaller}

-   Example of Boolean Search

    ![](images/clipboard-953469533.png)
    
:::notes
Let’s
take a look
at an example of a Boolean search. We’re going to use as our search
‘information AND retrieval NOT data.

AND in a Boolean search serves as an intersection between the terms, meaning that
any documents that are retrieved
have to
include both ‘information’ and ‘retrieval,’
otherwise the result set will not include that
particular document.
The OR serves as a union, meaning that any of the two terms
have to
be present
within the document within the OR relationship or the
document
would not be
returned in the retrieval set.

The NOT serves as a complement, or we might think of it as a way of filtering out
those documents that we do
not
want to see retrieved.

So, in this
particular search
we have ‘information AND retrieval NOT data.’ In
document A, we have ‘information’ and ‘data.’ So, with the NOT serving as a
complement or a filtering function, document A would not be retrieved.

Document B includes ‘information’ and ‘retrieval,’ which satisfies our first criteria, and
‘IR.’ Now, we don’t have the concept of IR as stated as the acronym as part of our
retrieval, but because this document includes our two terms, ‘information’ and
‘retrieval,’ and it does not include the term ‘data,’ this document would be retrieved.
Document C ‘information’ and ‘data’ and ‘retrieval’ would, again, not be retrieved
because it includes the term ‘data.’

And then, in Document
D
‘information’ and ‘retrieval’ and ‘book’ would be returned
because it includes our terms ‘information’ AND ‘retrieval.
:::

